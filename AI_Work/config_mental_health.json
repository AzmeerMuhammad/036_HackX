{
  "model": {
    "base_model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    "use_peft": true,
    "load_in_8bit": true,
    "load_in_4bit": false
  },
  "lora": {
    "r": 8,
    "lora_alpha": 32,
    "lora_dropout": 0.1,
    "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  },
  "training": {
    "output_dir": "models/mental_health_empathetic_model",
    "num_train_epochs": 3,
    "per_device_train_batch_size": 4,
    "per_device_eval_batch_size": 4,
    "gradient_accumulation_steps": 4,
    "learning_rate": 2e-4,
    "warmup_ratio": 0.1,
    "weight_decay": 0.01,
    "lr_scheduler_type": "cosine",
    "logging_steps": 10,
    "save_steps": 500,
    "eval_steps": 500,
    "save_total_limit": 3,
    "evaluation_strategy": "steps",
    "save_strategy": "steps",
    "load_best_model_at_end": true,
    "metric_for_best_model": "eval_loss",
    "greater_is_better": false,
    "fp16": true,
    "bf16": false,
    "gradient_checkpointing": true,
    "max_steps": -1,
    "report_to": "none"
  },
  "data": {
    "max_input_length": 1024,
    "max_output_length": 512,
    "train_split": 0.9,
    "val_split": 0.05,
    "test_split": 0.05,
    "empathetic_dialogues_path": "data/empatheticdialogues/empatheticdialogues"
  },
  "generation": {
    "max_new_tokens": 256,
    "temperature": 0.7,
    "top_p": 0.9,
    "top_k": 50,
    "num_beams": 4,
    "length_penalty": 1.2,
    "early_stopping": true,
    "do_sample": true,
    "repetition_penalty": 1.1
  },
  "seed": 42
}

